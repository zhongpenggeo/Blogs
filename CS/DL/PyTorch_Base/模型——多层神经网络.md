而不同的层数和每层的参数对模型的影响非常大，我们看看这个网站的 [demo](http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html)

#### 为什么需要激活函数

比如一个两层的神经网络，使用 A 表示激活函数，那么
$$
y = w_2 A(w_1 x)
$$

如果我们不使用激活函数，那么神经网络的结果就是
$$
y = w_2 (w_1 x) = (w_2 w_1) x = \bar{w} x
$$

可以看到，我们将两层神经网络的参数合在一起，用 $\bar{w}$ 来表示，两层的神经网络其实就变成了一层神经网络，只不过参数变成了新的 $\bar{w}$，所以如果不使用激活函数，那么不管多少层的神经网络，$y = w_n \cdots w_2 w_1 x = \bar{w} x$，就都变成了单层神经网络，所以在每一层我们都必须使用激活函数。

使用了激活函数之后，神经网络可以通过改变权重实现任意形状，越是复杂的神经网络能拟合的形状越复杂，这就是著名的神经网络万有逼近定理。



PyTorch 提供了两个模块来帮助我们构建模型，一个是Sequential，一个是 Module。

Sequential 允许我们构建序列化的模块，而 Module 是一种更加灵活的模型定义方式，我们下面分别用 Sequential 和 Module 来定义上面的神经网络。

#### Sequential

```python
# Sequential
seq_net = nn.Sequential(
    nn.Linear(2, 4), # PyTorch 中的线性层，wx + b
    nn.Tanh(),
    nn.Linear(4, 1)
)

# 序列模块可以通过索引访问每一层

seq_net[0] # 第一层
```

参考： https://wizardforcel.gitbooks.io/learn-dl-with-pytorch-liaoxingyu/3.3.html



#### Module

```python
class module_net(nn.Module):
    def __init__(self, num_input, num_hidden, num_output):
        super(module_net, self).__init__()
        self.layer1 = nn.Linear(num_input, num_hidden)

        self.layer2 = nn.Tanh()

        self.layer3 = nn.Linear(num_hidden, num_output)

    def forward(self, x):
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        return x
    
mo_net = module_net(2, 4, 1)
# 访问模型中的某层可以直接通过名字

# 第一层
l1 = mo_net.layer1
print(l1)
# Linear(in_features=2, out_features=4)

# 打印出第一层的权重
print(l1.weight)

# 定义优化器
optim = torch.optim.SGD(mo_net.parameters(), 1.)

# 我们训练 10000 次
for e in range(10000):
    out = mo_net(Variable(x))
    loss = criterion(out, Variable(y))
    optim.zero_grad()
    loss.backward()
    optim.step()
    if (e + 1) % 1000 == 0:
        print('epoch: {}, loss: {}'.format(e+1, loss.data[0]))
        
# 保存模型
torch.save(mo_net.state_dict(), 'module_net.pth')
```

#### 其他方法1

```python
from torch import nn
from collections import OrderedDict

class Net(nn.Module):
    
    def __init__(self, n_layers):
        
        super().__init__()
        
        layers = OrderedDict()
        for i in range(n_layers):
            layers[str(i)] = nn.Linear(5,5)
            
        self.layers = nn.Sequential(layers)
        print(self)
        
Net(n_layers=3)
```

#### 其他方法2

```python
class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.net = nn.Sequential()
        self.net.add_module('linear_layer_1', nn.Linear(2, 20))
        self.net.add_module('tanh_layer_1', nn.Tanh())
        for num in range(2,5):
            self.net.add_module('linear_layer_%d' %(num), nn.Linear(20, 20))
            self.net.add_module('tanh_layer_%d' %(num), nn.Tanh())
        self.net.add_module('linear_layer_50', nn.Linear(20, 1))

    def forward(self, x):
        return self.net(x)
```

#### 其他方法3

```python
class NeuralNetwork(nn.Module):
    """
    An implementation of a fully connected feed forward 
    Neural network in pytorch. 
    """
    def __init__(self, layersizes=[1, 1], 
                 activation=torch.relu,
                 final_layer_activation=None):
        """
        INPUTS:
            layersizes <list/tuple>: An iterable ordered object containing
                                 the sizes of the tensors from the
                                 input layer to the final output. 
                                 (See example below). 
            activation <callable>: A python callable through which
                                    torch backpropagation is possible.
            final_layer_activation <callable>: A python callable for 
                                    the final layer activation function.
                                    Default: None (for regression problems)
        
        EXAMPLE: 
            To define a NN with an input of size 2, 2 hidden layers of size 
            50 and 50, output of size 1, with tanh activation function: 
            >> layers = [2, 50, 50, 1]
            >> neuralnet = NeuralNet(layers, activation=torch.tanh)
            >> x = torch.randn(100, 2)   # 100 randomly sampled inputs 
            >> output = neuralnet(x)  # compute the prediction at x.
        
        Inheriting from nn.Module ensures that all
        NN layers defined within the __init__ function are captured and 
        stored in an OrderedDict object for easy accesability.
        """
        super(NeuralNetwork, self).__init__()
        self.layersizes = layersizes
        self.input_dim = self.layersizes[0]
        self.hidden_sizes = self.layersizes[1:-1]
        self.output_dim = self.layersizes[-1]
        self.activation = activation
        self.final_layer_activation = final_layer_activation
        if self.final_layer_activation is None:
            self.final_layer_activation = nn.Identity()
        self.nlayers = len(self.hidden_sizes) + 1
        self.layernames = [] ## Dictionary to store all the FC layers 
        
        # define FC layers
        for i in range(self.nlayers):
            layername = 'fc_{}'.format(i+1)
            layermodule = nn.Linear(self.layersizes[i], self.layersizes[i+1])
            self.layernames.append(layername)
            setattr(self, layername, layermodule)
        
    def forward(self, x):
        """
        Implement the forward pass of the NN. 
        """
        for i, layername in enumerate(self.layernames):
            fclayer = getattr(self, layername)
            x = fclayer(x)
            if i == self.nlayers - 1:
                x = self.final_layer_activation(x)
            else:
                x = self.activation(x)
        return x


# # sanity check 
# net = NeuralNetwork([1, 40, 40, 40, 1], torch.tanh)
# x = torch.linspace(0., 1., 100)[:, None]
# for m in net.modules():
#     print(m)

net = NeuralNetwork(layersizes=layersizes, 
                    activation=activation)
                    
```


## 模型中的梯度grad

在运行中，如果输入中有一个需要求导，那么输出一定需要求导 。

模型中的weight参数默认下requires_grad=True；

#### 模型经过后向传播计算的梯度是否保存在梯度中？

```python
import torch
import torch.nn.functional as F

net1 = torch.nn.Linear(1, 1)   
loss_fcn = torch.nn.BCELoss()

x = torch.zeros((1,1))
y = F.sigmoid(net1(x))

loss = loss_fcn(y, x)
loss.backward()

for param in net1.named_parameters():
    print(param[0], param[1].grad)
#weight tensor([[0.]])
#bias tensor([0.2995])

```
所以通过.grad属性可以调用tensor的梯度值。
但是，中间变量的梯度在使用之后就释放了，需要色湖之retain_grad()函数实现生命保留的梯度值
```python
net1 = torch.nn.Linear(1, 1)

loss_fcn = torch.nn.BCELoss()
x = torch.zeros((1,1))
y = F.sigmoid(net1(x))
loss = loss_fcn(y, x)
loss.backward()
print(y.grad)
#None

y = F.sigmoid(net1(x))
y.retain_grad()
loss = loss_fcn(y, x)
loss.backward()
print(y.grad)
#tensor([[1.9580]])
```
#### 参数weight
模型训练过程中参数时保留在模型中，通过model.parameters()或者model.named_parameters()获取
```python
import torch
import torch.nn.functional as F

net = torch.nn.Linear(1, 1)   
for param in net.named_parameters():
    print(param[0], param[1].data)
#weight tensor([[-0.4321]])
#bias tensor([0.6777])

for param in net.parameters():
    print(param.data)
#tensor([[-0.4321]])
#tensor([0.6777])
```
通过上面的例子也可以看出，模型在声明的时候，就会自动初始化参数值。
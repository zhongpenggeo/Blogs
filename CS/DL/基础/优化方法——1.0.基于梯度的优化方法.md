我们把要最小化或最大化的函数称为`目标函数`(objective function)或者准则(criterion)。当我们对其进行最小化时，也把它称为`代价函数`(cost function)、`损失函数`(loss function)或`误差函数`(error function)。

通常用一个上标\*表示最小化或最大化函数$\boldsymbol{x}$的值，如$\boldsymbol{X}^*=\arg\min f(\boldsymbol{x}$)

导数对于最小化一个函数很有用，指导我们如何更改x来改善函数。

当$f'(x)=0$，导数无法提供往哪个方向移动的信息，这被称为`临界点`或`驻点`。一个`局部极小点`意味着这个点的$f(x)$小于所有的临近点，因此无法通过移动无穷小的步长来减小$f(x)$。有些临界点既不是极小点也不是极大点，这被称为`鞍点`。  

针对具有多为输入的函数，使用`偏导数`$\frac{\partial}{\partial x_i}f(\boldsymbol{x})$。`梯度`则相对一个向量求导的导数：$f$的梯度是包含所有偏导数的向量，记为$\nabla_x f(\boldsymbol{x})$。梯度的第$i$个元素时$f$关于$x_i$的偏导数

在 $\boldsymbol{u}$（单位向量）方向的`方向导数`是函数 $f$ 在 $\boldsymbol{u}$ 方向的斜率。换句话说，方向导数是函数$f(\boldsymbol{x}+\alpha\boldsymbol{u})$关于$\alpha$的导数（在$\alpha=0$时取得）。使用链式法则，当$\alpha=0$时，$\frac{\partial}{\partial \alpha}f(\boldsymbol{x}+\alpha\boldsymbol{u})=\boldsymbol{u}^T\nabla_xf(\boldsymbol{x})$。梯度方向为方向导数最大的方向，其模为方向导数的绝对值。

为了最小化$f$，我们希望找到使$f$下降得最快的方向
$$
\min_{\boldsymbol{u},\boldsymbol{u}^T\boldsymbol{u}=1} \boldsymbol{u}^T\nabla_xf(\boldsymbol{x})\\
=\min_{\boldsymbol{u},\boldsymbol{u}^T\boldsymbol{u}=1} ||\boldsymbol{u}||_2\, ||\nabla_xf(\boldsymbol{x})||_2 \cos\theta
$$
其中，$\theta$使$\boldsymbol{u}$与梯度的夹角。将$||\boldsymbol{u}||_2=1$代入，并忽略与$\boldsymbol{u}$无相关项，就可以简化得到$$\min_{\boldsymbol{u}}\cos\theta$$,这在$\boldsymbol{u}$与梯度方向相反时（$\theta=n\pi$）取得最小。所以我们在**负梯度方向上移动**可以减小$f$。这杯称为`最速下降法`或`梯度下降`
$$
\boldsymbol{x}'=\boldsymbol{x}-\epsilon\nabla_\boldsymbol{x}f(\boldsymbol{x})
$$
其中，$\epsilon$为`学习率`。还有一种方法是根据几个$\epsilon$计算$f(\boldsymbol{x}-\epsilon\nabla_\boldsymbol{x}f(\boldsymbol{x}))$，并选择其中能产生最小目标函数值的$\epsilon$，这种策略被称为`线搜索`。

### Jacobian和 Hessian矩阵

计算输入和输出都为向量的函数的所有偏导数，所有这种偏导数组成`Jacobian`矩阵。

比如我们有一个函数：$f:\mathbb{R}^M\rightarrow \mathbb{R}^n$，$f$的Jacobian矩阵$\boldsymbol{J}\in \mathbb{R}^{n\times m}$ 定义为
$$
J_{i,j}=\frac{\partial}{\partial x_j}f(x_i)
$$


二阶导数合并构成的为`Hessian`矩阵
$$
\boldsymbol{H}(f)(\boldsymbol{x})_{i,j}=\frac{\partial^2}{\partial x_i \partial x_j}f(\boldsymbol{x})
$$
Hessian 等价于梯度的 Jacobian 矩阵  

微分算子在任何二阶偏导连续的点处可以交出：
$$
\frac{\partial^2}{\partial x_i \partial x_j}f(\boldsymbol{x})=\frac{\partial^2}{\partial x_j \partial x_i}f(\boldsymbol{x})
$$
也就是$H_{i,j}=H_{j,i}$，因此Hessian矩阵在这些点上是对称的。

因为 Hessian 矩阵是**实对称**的，我们可以将其分解成一组实特征值和一组特征向量的正交基。在特定方向$\boldsymbol{d}$上的二阶导数可以写成$\boldsymbol{d}^T\boldsymbol{Hd}$。当 $\boldsymbol{d}$ 是 $\boldsymbol{H}$ 的一个特征向量时，这个方向的二阶导数就是对应的特征值。对于其他的方向 $\boldsymbol{d}$，方向二阶导数是所有特征值的加权平均，权重在 0 和 1 之间，且与 $\boldsymbol{d}$ 夹角越小的特征向量的权重越大。最大特征值确定最大二阶导数，最小特征值确定最小二阶导数。

在当前点 $\boldsymbol{x}^{(0)}$ 处做函数 $f(\boldsymbol{x})$ 的近似二阶泰勒计数
$$
f(\boldsymbol{x}) = f(\boldsymbol{x}^{(0)}) + (\boldsymbol{x}-\boldsymbol{x}^{(0)})^T\boldsymbol{g} + \frac{1}{2}(\boldsymbol{x}-\boldsymbol{x}^{(0)})^T\boldsymbol{H}(\boldsymbol{x}-\boldsymbol{x}^{(0)})
$$
其中， $\boldsymbol{g}$ 是梯度， $\boldsymbol{H}$ 是 $\boldsymbol{x}^{(0)}$ 处的Hessian矩阵。如果使用学习率 $\epsilon$，那么新的点 $\boldsymbol{x}$ 将会是 $\boldsymbol{x}^{(0)}-\epsilon\boldsymbol{g}$，代入上式，可得
$$
f(\boldsymbol{x}^{(0)}-\epsilon\boldsymbol{g}) = f(\boldsymbol{x}^{(0)}) -\epsilon\boldsymbol{g}^T\boldsymbol{g} + \frac{1}{2}\epsilon^2 \boldsymbol{g}^T\boldsymbol{H}\boldsymbol{g}
$$
其中有 3 项：**函数的原始值、函数斜率导致的预期改善、函数曲率导致的校正**。当
最后一项太大时， 梯度下降实际上是可能向上移动的。当 $\boldsymbol{g}^T\boldsymbol{H}\boldsymbol{g}$ 为零或负时，近似
的泰勒级数表明增加 $\epsilon$ 将永远使 $f$ 下降。在实践中， 泰勒级数不会在 $ϵ$ 大的时候也
保持准确，因此在这种情况下我们必须采取更启发式的选择。当 $\boldsymbol{g}^T\boldsymbol{H}\boldsymbol{g}$ 为正时，通
过计算可得，使近似泰勒级数下降最多的最优步长为
$$
\epsilon^* = \frac{\boldsymbol{g}^T\boldsymbol{g}}{\boldsymbol{g}^T\boldsymbol{H}\boldsymbol{g}}
$$

最坏的情况下， $g$ 与 $H$ 最大特征值 $λ_{max}$ 对应的特征向量对齐，则最优步长是 $\frac{1}{λ_{max}}$ 。
我们要最小化的函数能用二次函数很好地近似的情况下， Hessian 的特征值决定了学习率的量级。  

二阶导数还可以被用于确定一个临界点是否是局部极大点、 局部极小点或鞍点。  

pass

### 约束优化

pass

KKT条件

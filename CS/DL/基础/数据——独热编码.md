#### 连续特征与离散特征

拿到获取的原始特征，必须对每一特征分别进行归一化，比如，特征A的取值范围是[-1000,1000]，特征B的取值范围是[-1,1].如果使用logistic回归，w1*x1+w2*x2，因为x1的取值太大了，所以x2基本起不了作用。所以，必须进行特征的归一化，每个特征都**单独进行归一化**。

对于连续特征：可以做各种归一化（rescale bound，标准差）

对于离散特征：对于离散的特征基本就是按照**one-hot（独热）**编码，该离散特征有多少取值，就用**多少维**来表示该特征。

#### 独热编码

假如有三种颜色特征：红、黄、蓝。 在利用机器学习的算法时一般需要进行向量化或者数字化。那么你可能想令 红=1，黄=2，蓝=3. 那么这样其实实现了标签编码，即给不同类别以标签。然而这意味着机器可能会学习到“红<黄<蓝”，但这并不是我们的让机器学习的本意，只是想让机器区分它们，并无大小比较之意。所以这时标签编码是不够的，需要进一步转换。因为有三种颜色状态，所以就有3个比特。即红色：1 0 0 ，黄色: 0 1 0，蓝色：0 0 1 。如此一来每两个向量之间的距离都是根号2，在向量空间距离都相等，所以这样不会出现偏序性，基本不会影响基于向量空间度量算法的效果。

自然状态码为：000,001,010,011,100,101

独热编码为：000001,000010,000100,001000,010000,100000

#### 为什么要独热编码

使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，会让特征之间的**距离计算更加合理**。离散特征进行one-hot编码后，编码后的特征，其实每一维度的特征都可以看做是**连续的特征**。就可以跟对连续型特征的**归一化**方法一样，对每一维特征进行归一化。比如归一化到[-1,1]或归一化到均值为0,方差为1。

为什么特征向量要映射到欧式空间？

将离散特征通过one-hot编码映射到欧式空间，是因为在回归、分类、聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。

#### 优缺点

- 优点：独热编码解决了分类器不好处理属性数据的问题，在一定程度上也起到了**扩充特征**的作用。它的值只有0和1，不同的类型存储在垂直的空间。
- 缺点：当类别的数量很多时，**特征空间**会变得非常大。在这种情况下，一般可以用PCA来**减少维度**。而且one hot encoding+PCA这种组合在实际中也非常有用。

#### 什么情况下(不)用独热编码？

- 用：独热编码用来解决**类别型数据**的离散值问题，
- 不用：将离散型特征进行one-hot编码的作用，是为了**让距离计算更合理**，但如果特征是离散的，并且不用one-hot编码就可以很合理的计算出距离，那么就没必要进行one-hot编码。 有些基于**树的算法**在处理变量时，并不是基于**向量空间**度量，数值只是个类别符号，即没有偏序关系，所以不用进行独热编码。 Tree Model不太需要one-hot编码： 对于决策树来说，one-hot的本质是**增加树的深度**。

总的来说，要是one hot encoding的类别数目不太多，建议优先考虑。
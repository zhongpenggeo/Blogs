贝叶斯神经网络，则是一种解决传统神经网络中固定参数难以解释模型不确定性而导致的**过拟合和泛化能力较弱**的方法。

（注意与`贝叶斯网络`的区分。贝叶斯网络时一种在图模型中应用贝叶斯公式的方法）

### 0. 介绍

传统的神经网络中，我们认为模型参数$\mathbf{w}$是固定的，存在最优参数 $\mathbf{w}^*$ ，然后给模型参数赋一组初值$\mathbf{w_0}$ ，然后基于观测数据集 $D$训练模型使$\mathbf{w}_0$不断逼近$\mathbf{w}^*$，这个过程就是利用最大似然参数估计和梯度下降等算法学习最优模型参数的过程，学习的微观过程就是最大化给定参数$\mathbf{w}$在训练集$D$上的似然函数$p(D|\mathbf{w})$，每次训练我们都认为$\mathbf{w}$是定值.

但这样容易在有限的训练数据上导致过拟合。

一种主流方法是在传统神经网络引入 Dropout，通过在训练过程中随机丢弃某些单元增加模型的不确定性来有效减少过拟合现象。引入正则化在本质上也是通过降低模型的复杂度来减小过拟合。

另一方面，尝试在参数上引入不确定性，于是不再把$\mathbf{w}$当作固定参数，而是当作一个概率分布，很自然地，为了最大化不确定性(?)，我们让$\mathbf{w}$服从高斯分布。相对应地，此时$\mathbf{w}$的先验不再是简单的正则项，而是对应的共轭先验分布。高斯分布的共轭先验分布也应该是一个高斯分布，对应的后验分布也是一个高斯分布，我们最终目的就是找出这个后验高斯分布 $p(\mathbf{w}|D)$，同时还能利用参数的不确定性来度量模型的不确定性。



网络模型对于参数值的⾼度⾮线性意味着精确的贝叶斯⽅法不可⾏，而后验概率分布的对数⾮凸，正对应于误差函数中的多个局部极⼩值。对此我们需要借助其他方法来解决，比较常见的是变分推断 (variational inference) 法和拉普拉斯 (Laplace) 近似法。



参考：

[贝叶斯神经网络 Bayesian Neural Network: 变分法，拉普拉斯近似法 - PRML - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/373919999)
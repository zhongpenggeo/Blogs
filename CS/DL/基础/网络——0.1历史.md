
现代前馈网络的核心思想自 20 世纪 80 年代以来没有发生重大变化。仍然使用相同的反向传播算法和相同的梯度下降方法。 1986 年至 2015 年神经网络性能的大部分改进可归因于两个因素。首先，较大的数据集减少了统计泛化对神经网络的挑战的程度。第二，神经网络由于更强大的计算机和更好的软件基础设施已经变得更大。然而，**少量算法上的变化也显著改善了神经网络的性能**。  

其中一个算法上的变化是**用损失函数的交叉熵族替代均方误差**。 均方误差在 20 世纪 80 年代和 90 年代流行，但逐渐被交叉熵损失替代，并且最大似然原理的想法在统计学界和机器学习界之间广泛传播。使用交叉熵损失大大提高了具有 sigmoid 和 softmax 输出的模型的性能，而当**使用均方误差损失时会存在饱和和学习缓慢的问题**。  
另一个显著改善前馈网络性能的算法上的主要变化是使用**分段线性隐藏单元来替代 sigmoid 隐藏单元，例如用整流线性单元（ReLU）**。使用 $\max\{0,z\}$ 函数的整流在早期神经网络中已经被引入，并且至少可以追溯到认知机（Cognitron）和神经认知机(Neocognitron)(Fukushima, 1975, 1980)。这些早期的模型没有使用整流线性单元，而是将整流用于非线性函数。尽管整流在早期很普及，在 20 世纪 80 年代，整流很大程度上被 sigmoid 所取代，也许是因为当神经网络非常小时， sigmoid 表现更好。 这在 2009 年开始发生改变。 Jarrett et al. (2009b) 观察到，在神经网络结构设计的几个不同因素中 “使用整流非线性是提高识别系统性能的最重要的唯一因素”。   

对于小的数据集， Jarrett et al. (2009b) 观察到，使用整流非线性甚至比学习隐藏层的权重值更加重要。随机的权重足以通过整流网络传播有用的信息，允许在顶部的分类器层学习如何将不同的特征向量映射到类标识。

当有更多数据可用时，学习开始提取足够的有用知识来超越随机选择参数的性能。 Glorot et al. (2011a) 说明，在深度整流网络中的学习比在激活函数具有曲率或两侧饱和的深度网络中的学习更容易。  



参考：

《Deep Learning》花书 6.6节


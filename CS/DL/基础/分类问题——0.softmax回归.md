### 分类问题

#### softmax

在分类问题中，需要最小化输出 $o$ 和标签 $y$ 的差异，如果把分类问题当作向量值回归问题，还存在以下不足：

- 不能保证输出 $o_i$ 的累加和为1（我们希望概率和为1）
- 不能保证输出 $o_i$ 是非负

一种方案是假定 $y=o+\epsilon$, where $\epsilon_i \sim \mathcal{N}(0, \sigma^2)$。这被称为 [probit model](https://en.wikipedia.org/wiki/Probit_model), first introduced by Fechner ([1860](https://d2l.ai/chapter_references/zreferences.html#id67))，但是这个优化目标不好求解。

另一个方案是使用幂函数 $P(y = i) \propto \exp o_i$。为了使和为1，可以除以它们的和，这被称为归一化。
$$
\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o}) \quad \text{where}\quad \hat{y}_i = \frac{\exp(o_i)}{\sum_j \exp(o_j)}.
$$
更能重要的，不需要计算softmax来决定类被是的概率最高
$$
\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j.
$$

### 损失函数

#### Log-Likelihood

$$
P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).
$$

可以转换为
$$
-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)})
= \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)}),
$$
where for any pair of label $y$ and model prediction $\hat{y}$ over $q$ classes, the loss function $l$ is
$$
l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j.
$$
上式被称为交叉熵损失。

#### Softmax and Cross-Entropy Loss

$$
\begin{split}\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j \\
&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
\end{aligned}\end{split}
$$

对 $o_j$ 求导得到
$$
\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j.
$$
其梯度为观测的 $y$ 和估计的 $\hat{y}$ 之间的差异。这使得梯度很容易计算

考虑我们观察的不是单一的结果，而是整个结果的分布，即不是一个binary entries $(0,0,1)$，而是一个概率矢量 $(0.1,0.2,0.7)$。

这就引出了信息熵的理论

### 信息论

#### 熵

信息论的核心思想是：**量化数据中的信息内容**。，信息论中，该数值被称为分布 $P$ 的熵
$$
H[P] = \sum_j - P(j) \log P(j).
$$
信息论的基本定理之一指出，为了对从分布 $P$ 中随机抽取的数据进行编码， 我们至少需要 $H[P]$ “纳特（nat）”对其进行编码。 “纳特”相当于*比特*（bit），但是对数底为e而不是2。因此，一个纳特是 $\frac{1}{\log(2)} \approx 1.44$ 比特。

#### 信息量

压缩与预测有什么关系呢？ 想象一下，我们有一个要压缩的数据流。 如果我们很容易预测下一个数据，那么这个数据就很容易压缩。 为什么呢？ 举一个极端的例子，假如数据流中的每个数据完全相同，这会是一个非常无聊的数据流。 由于它们总是相同的，我们总是知道下一个数据是什么。 所以，为了传递数据流的内容，我们不必传输任何信息。也就是说，“下一个数据是xx”这个事件毫无信息量。

香农决定用信息量 $\log \frac{1}{P(j)} = -\log P(j)$来量化这种预测事情的惊异程度。当我们赋予一个事件较低的概率时，我们的惊异会更大，该事件的信息量也就更大。熵是当分配的概率真正匹配数据生成过程时的*信息量的期望*。

 我们可以把交叉熵想象为“主观概率为 $Q$ 的观察者在看到根据概率 $P$ 生成的数据时的预期惊异”。 当 $P=Q$ 时，交叉熵达到最低。 在这种情况下，从 $P$ 到 $Q$ 的交叉熵是 $H(P,P)=H(P)$。

简而言之，我们可以从两方面来考虑交叉熵分类目标： （i）最大化观测数据的似然；（ii）最小化传达标签所需的惊异。

#### 参考

[3.4. softmax回归 — 动手学深度学习 2.0.0 documentation (d2l.ai)](https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html#id10)
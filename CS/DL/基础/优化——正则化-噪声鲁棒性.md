### 噪声鲁棒性

对于某些模型而言，**向输入添加方差极小的噪声等价于对权重施加范数惩罚** (Bishop, 1995a,b)。在一般情况下，**噪声注入远比简单地收缩参数强大，特别是噪声被添加到隐藏单元时会更加强**
**大**。向隐藏单元添加噪声是值得单独讨论重要的话题。  

另一种正则化模型的噪声使用方式是将其加到权重。这项技术主要用于循环神经网络 (Jim et al., 1996; Graves, 2011)。这可以被解释为关于权重的贝叶斯推断的随机实现。贝叶斯学习过程将权重视为不确定的，并且可以通过概率分布表示这种不确定性。**向权重添加噪声是反映这种不确定性的一种实用的随机方法**。  

我们研究回归的情形，也就是训练将一组特征 $\boldsymbol{x}$ 映射成一个标量的函数 $\hat{y}(\boldsymbol{x})$，并使用最小二乘代价函数衡量模型预测值 $\hat{y}(\boldsymbol{x})$ 与真实值 $y$的误差：  
$$
J  = \mathbb{E}_{p(x,y)} [ (\hat{y}(\boldsymbol{x}) - y)^2]
$$
数据集包含 $m$ 个数据对；

现在我们假设对每个输入表示，网络权重添加随机扰动 $\epsilon_\boldsymbol{W} \sim \mathcal {N}(\boldsymbol{\epsilon};0,\eta \boldsymbol{I})$。将扰动模型即为 $\hat{y}_{\epsilon_\boldsymbol{W}}(\boldsymbol{x})$，目标函数变为
$$
\tilde{J}_\boldsymbol{W}  = \mathbb{E}_{p(x,y,\epsilon_\boldsymbol{W})} [ (\hat{y}_{\epsilon_\boldsymbol{W}}(\boldsymbol{x}) - y)^2]\\
= \mathbb{E}_{p(x,y,\epsilon_\boldsymbol{W})} [ \hat{y}^2_{\epsilon_\boldsymbol{W}}(\boldsymbol{x}) -2y\hat{y}_{\epsilon_\boldsymbol{W}}(\boldsymbol{x}) +y^2]
$$
对于小的$\eta$，最小化带权重噪声的$J$等同于最小化附加正则化项的 $J:\eta\mathbb{E}_{p(x,y)}[||\nabla_\boldsymbol{W}\hat{y}(\boldsymbol{x})||^2]$。这种形式的正则化鼓励参数进入权重小扰动对输出相对影响较小的参数空间区域，即：他推动模型进入对权重小的变化相对不敏感的区域，找到的点不是极小点，还是由平坦区域所包围的最小点。在简化的线性回归中（$\hat{y}(\boldsymbol{x})= \boldsymbol{W}^T\boldsymbol{x}+b$，正则化退化为$\eta\mathbb{E}_{p(x)}[||\boldsymbol{x}||^2]$，这与函数的参数无关，因此不会对 $\tilde{J}_\boldsymbol{W}  $ 关于模型参数的梯度有影响。  

### 向输出目标注入噪声

大多数数据集的 $y$ 标签都有一定错误，不利于最大化 $\log p(y|\boldsymbol{x})$，避免这种情况的一种方法是**显式地对标签上的噪声进行建模**。

比如，假定，对于一些小常数 $\epsilon$， 训练集标记 $y$ 是正确的概率是 $1-\epsilon$。该假设很容易解析地与代价函数结合，而不用显式地抽取噪声样本。比如`标签平滑`通过把确切分类目标从0和1换成$\frac{\epsilon}{k-1}$和 $1-\epsilon$，正则化具有 $k$ 个输出的softmax函数的模型。

使用softmax函数和明确目标的最大似然学习可能永远不会收敛，因为softmax函数永远无法真正预测 0 概率和 1 概率，导致会继续学习越来越大的权重。使用权重衰减等其他正则化策略能够防止这种情况。

标签平滑的优势是能够防止模型追求确切的概率而不影响模型学习正确分类。
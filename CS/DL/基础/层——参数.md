### 1. 参数访问

可以通过索引来访问模型的任意层。 这就像模型是一个列表一样，每层的参数都在其属性中。 如下所示，我们可以检查第二个全连接层的参数。

```python
print(net[2].state_dict())
```

```
OrderedDict([('weight', tensor([[ 0.0251, -0.2952, -0.1204,  0.3436, -0.3450, -0.0372,  0.0462,  0.2307]])), ('bias', tensor([0.2871]))])
```

#### 1.1 目标参数

```python
print(type(net[2].bias))
print(net[2].bias)
print(net[2].bias.data)
```

#### 1.2 一次访问所有参数

#### 1.3 从嵌套块收集参数

### 2. 初始化

#### 2.1 内置初始化

默认情况下，PyTorch会根据一个范围均匀地初始化权重和偏置矩阵， 这个范围是根据输入和输出维度计算出的。 PyTorch的`nn.init`模块提供了多种预置初始化方法。

#### 2.2 自定义初始化

### 3. 参数绑定

多个层间共享参数

```python
# 我们需要给共享层一个名称，以便可以引用它的参数
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8, 1))
net(X)
# 检查参数是否相同
print(net[2].weight.data[0] == net[4].weight.data[0])
net[2].weight.data[0, 0] = 100
# 确保它们实际上是同一个对象，而不只是有相同的值
print(net[2].weight.data[0] == net[4].weight.data[0])
```

```
tensor([True, True, True, True, True, True, True, True])
tensor([True, True, True, True, True, True, True, True])
```

当参数绑定时，梯度会发生什么情况？ 

答案是由于模型参数包含梯度，因此在反向传播期间第二个隐藏层 （即第三个神经网络层）和第三个隐藏层（即第五个神经网络层）的梯度会加在一起。
![image-20220321194039418](../imags/image-20220321194039418.png)

### GPT-1

用transformer的**解码器**，在没有标签的语言模型上做预训练，然后再拿来微调做子任务

### BERT

用transformer的**编码器**，用更大的数据集

### GPT-2

继续用解码器，更大的数据集，可以做zero-shot

### GPT-3

模型和数据集都比GPT-2大10倍
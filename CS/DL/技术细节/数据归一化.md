## 为什么要归一化

如果你的输入数据全是正数或全是负数，那么根据激活函数的特性，会出现锯齿形的loss，

但如果映射为接近为0的正负数，结果会更好；



2.

输入的feature可能有尺度效应，可以去除

3.

![image-20201127214353083](../imags/image-20201127214353083.png)

归一化之后，梯度自然也就变小了。

4.

如果输入数据尺度不一样，导致梯度大小不一，学习率则很难兼顾。



参考：[Why Data should be Normalized before Training a Neural Network | by Timo Stöttner | Towards Data Science](https://towardsdatascience.com/why-data-should-be-normalized-before-training-a-neural-network-c626b7f66c7d)



## 常见的数据归一化方法

#### min-max标准化

$$
X^*=\frac{x-min}{max-min}
$$

or映射到[-1,1]

$$
X^*=\frac{x-mean}{max-min}
$$

当有新的数据加入时，需要重新定义

#### z-score 标准化(zero-mean normalization)
x* = (x - μ ) / σ

z-score标准化方法适用于属性A的最大值和最小值未知的情况，或有超出取值范围的离群数据的情况。该种标准化方式要求原始数据的分布可以近似为高斯分布，否则效果会变得很糟糕。

#### log函数转换
$$
X^*=log_{10}(x)/log_{10}(max)
$$

#### atan 函数
### 为什么要正则化

防止过拟合，提高泛化能力。

#### 泛化

1. 训练数据和真实数据间的差异，一般要高正训练和测试集的数据分布相同，
2. 模型稀疏性，正如奥斯姆的剪刀指出的，面对不同的解释时，最简单的解释是最好的解释。在机器学习中，具有泛化能力的模型中应该有很多参数是接近0的。而在深度学习中，则是待优化的矩阵应该对稀疏性有偏好性。
3. **生成模型中的高保真能力**
4. 有**效的忽视琐碎的特征**，或者说在无关的变化下都能找到相同的特征
5. **风险最小化**



### 正则化

都是针对模型中参数过大的问题引入惩罚项。而在深度学习中，要优化的变成了一个个矩阵，参数变得多出了几个数量级，过拟合的可能性也相应的提高了。而要惩罚的是神经网络中每个神经元的权重大小，从而避免网络中的神经元走极端抄近路。

假定存在正实数C>0：

$$
\mid\mid W_i\mid\mid_2\leq C
$$

L2正则化，又称**权重衰减**（weight decay）关注的是权重平方和的平方根，是要网络中的权重接近0但不等于0，而在L1正则中，要关注的是权重的绝对值，权重可能被压缩成0。在深度学习中，L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。神经网络需要每一层的神经元尽可能的提取出有意义的特征，而这些特征不能是无源之水，因此L2正则用的多一些。
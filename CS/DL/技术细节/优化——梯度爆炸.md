### 1.  问题

在深层网络或循环神经网络中，误差梯度可在更新中累积，变成非常大的梯度，然后导致网络权重的大幅更新，并因此使网络变得不稳定。在极端情况下，权重的值变得非常大，以至于溢出，导致 NaN 值。

### 2. 现象

细微现象:

- 模型无法从训练数据中获得更新（如低损失）。
- 模型不稳定，导致更新过程中的损失出现显著变化。
- 训练过程中，模型损失变成 NaN。

明显现象

- 训练过程中模型梯度快速变大。
- 训练过程中模型权重变成 NaN 值。
- 训练过程中，每个节点和层的误差梯度值持续超过 1.0。

### 3. 解决方法

#### 3.1 重新设计网络

1. 减少层数
2. 使用更小的批尺寸对网络训练
3. 在循环神经网络中，训练过程中在更少的先前时间步上进行更新（沿时间的截断反向传播，truncated Backpropagation through time）可以缓解梯度爆炸问题。

#### 3.2 使用ReLU激活函数

在深度多层感知机神经网络中，梯度爆炸的发生可能是因为激活函数，如之前很流行的 Sigmoid 和 Tanh 函数。

使用 ReLU 激活函数可以减少梯度爆炸。采用 ReLU 激活函数是最适合隐藏层的新实践。

#### 3.3 使用长短期记忆网络

#### 3.4 使用梯度截断(gradient clipping)

在非常深且批尺寸较大的多层感知机网络和输入序列较长的 LSTM 中，仍然有可能出现梯度爆炸。如果梯度爆炸仍然出现，你可以在训练过程中检查和限制梯度的大小。这就是梯度截断。

在 Keras 深度学习库中，你可以在训练之前设置优化器上的 clipnorm 或 clipvalue 参数，来使用梯度截断。

默认值为 clipnorm=1.0 、clipvalue=0.5。详见：https://keras.io/optimizers/

#### 3.5 使用权重正则化(weight regularization)

权重正则化，通常使用的是 L1 惩罚项（权重绝对值）或 L2 惩罚项（权重平方）。



参考: https://www.jiqizhixin.com/articles/2017-12-21-14
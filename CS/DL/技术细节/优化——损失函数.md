损失函数的优化



### 损失函数与学习率

loss 除以 C，学习率乘以 C，二者可以完全抵消，跟原来是等价的。



### 经验结论

#### skip connection网络更平滑，容易收敛

![img](../imags/v2-182acea79e399c61f7e0205d191d9848_720w.jpg)

#### 更深的模型它的loss curvature/geometry 更 sharp

更不容易收敛到 local minimum，且loss geometry 的 flatness 在经验上跟模型的generalization ability 成正比：loss curvature 越平坦，模型越容易 generalization，在test set上的表现越好

![img](../imags/v2-f8e3c8ed7b738de216590d7a159aaa28_720w.jpg)

#### 模型越宽，损失函数越flat

![img](../imags/v2-a7f80a1d5d45ed17b4ddfaef2032647a_720w.jpg)







[神经网络如何设计自己的loss function，如果需要修改或设计自己的loss，需要遵循什么规则？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/59797824)
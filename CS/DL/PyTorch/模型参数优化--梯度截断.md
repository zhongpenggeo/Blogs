两种方式

#### clip_grad_norm

`clip_grad_norm` (which is actually deprecated in favor of `clip_grad_norm_` following the more consistent syntax of a trailing `_` when in-place modification is performed) clips the norm of the *overall* gradient by concatenating all parameters passed to the function, as can be seen from [the documentation](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html):

```python
optimizer.zero_grad()        
loss, hidden = model(data, hidden, targets)
loss.backward()

torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)
optimizer.step()
```

#### [`clip_grad_value_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html)

```python
clip_grad_value_(model.parameters(), clip_value)
```

#### clam_

```python
if param.grad is not None:
                param.grad.data.clamp_(-grad_clip, grad_clip)
```



参考:https://stackoverflow.com/questions/54716377/how-to-do-gradient-clipping-in-pytorch
#### 1. 不确定性来源

1. 建模系统内在的随机性
2. 观测的不完全性
3. 不完全建模：舍弃了模型观测星系的模型

#### 随机变量

random variable，随机选取不同值的变量，可以离散或利纳许

#### 概率分布

probability distribution）用来描述随机变量或一簇随机变量在每一个可能取到的状态的可能性大小。我们描述概率分布的方式取决于随机变量是离散 的还是连续的。

#### 离散型变量和概率质量函数

**离散型变量**的概率分布可以用 **概率质量函数**（probability mass function, PMF） 1 来描述。我们通常用大写字母 P 来表示概率质量函数。通常每一个随机变量都会有 一个不同的概率质量函数，并且读者必须根据随机变量来推断所使用的 PMF，而不 是根据函数的名称来推断；例如，P(x) 通常和 P(y) 不一样。

有时我们会先定义一个随机变量，然后用 ∼ 符号来说明 它遵循的分布：x ∼ P(x)。

概率质量函数可以同时作用于多个随机变量。这种多个变量的概率分布被称 为 **联合概率分布**（joint probability distribution）。P(x = x, y = y) 表示 x = x 和 y = y **同时发生**的概率。我们也可以简写为 P(x, y)。

#### 连续型变量和概率密度函数

当我们研究的对象是连续型随机变量时， 我们用 概率密度函数（probability density function, PDF）而不是概率质量函数来描述它的概率分布

概率密度函数 p(x) 并没有直接对特定的状态给出概率，相对的，它给出了落在 面积为 δx 的无限小的区域内的概率为 p(x)δx。

#### 边缘概率

有时候，我们知道了一组变量的联合概率分布，但想要了解其中一个子集的概 率分布。这种定义在子集上的概率分布被称为 **边缘概率分布**（marginal probability distribution）。

例如，假设有离散型随机变量 x 和 y，并且我们知道 P(x, y)。我们可以依据下 面的 **求和法则**（sum rule）来计算 P(x)：

‘‘边缘概率’’ 的名称来源于手算边缘概率的计算过程。当 P(x, y) 的每个值被写 在由每行表示不同的 x 值，每列表示不同的 y 值形成的网格中时，对网格中的每行 求和是很自然的事情，然后将求和的结果 P(x) 写在每行右边的纸的边缘处。

对于连续型变量，我们需要用积分替代求和：

#### 条件概率

在很多情况下， 我们感兴趣的是某个事件， 在给定其他事件发生时出现的 概率。这种概率叫做条件概率。我们将给定 x = x，y = y 发生的条件概率记为 P(y = y | x = x)。

#### 正态分布

采用正态分布在很多应用中都是一个明智的选择。当我们由于缺乏关于某个实 数上分布的先验知识而不知道该选择怎样的形式时，正态分布是默认的比较好的选 择，其中有两个原因。

第一，我们想要建模的很多分布的真实情况是比较接近正态分布的。 中心极限 定理（central limit theorem）说明很多独立随机变量的和近似服从正态分布。这意 味着在实际中，很多复杂系统都可以被成功地建模成正态分布的噪声，即使系统可 以被分解成一些更结构化的部分。

第二，在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大 的不确定性。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。

#### Dirac 分布和经验分布

我们希望概率分布中的所有质量都集中在一个点上。这可以通 过 **Dirac delta 函数**（Dirac delta function）δ(x) 定义概率密度函数来实现：

Dirac delta 函数被定义成在除了 0 以外的所有点的值都为 0，但是积分为 1（类似于脉冲函数）。

Dirac 分布经常作为 经验分布（empirical distribution）的一个组成部分出现：

经验分布将概率密度 这些点是给定的 数据集或者采样的集合。只有在定义连续型随机变量的经验分布时，Dirac delta 函 数才是必要的。对于离散型随机变量， 情况更加简单：经验分布可以被定义成一 个 Multinoulli 分布，对于每一个可能的输入，其概率可以简单地设为在训练集上那 个输入值的 **经验频率**（empirical frequency）。



最大似然估计：

给定一个概率分布D，我们已知其概率密度函数$f_D$和一个分布参数$\theta$，

然后我们从中抽出一个n个值的采样$X1,X2...Xn$，利用$f_D$估计他的似然函数L；

当我们获得$X1,...Xn$时，就可能得到一个$\theta$使L取到最大值，这时的$\theta$为最大似然估计。

因此，最大似然估计时样本的函数。

举例：

如果一个盒子里有三个硬币（重量不同），抛出正面的概率分别时：1/3、1/2、2/3.

然后你取出其中一个，抛80次，得到49次正面，31次方面，估计这是哪一个硬币？

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/345e94cfdbd3816d8f4ef2097fd5074c23e037be)

可以看到，p=2/3那个让似然函数取得最大值,这就是p的最大似然估计

参考：[https://medium.com/@2681506/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E7%8E%87%E8%AE%BA-cac83ecff91c](https://medium.com/@2681506/深度学习概率论-cac83ecff91c)


### 共轭方向法

#### 共轭性

![img](./imags/%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D%E6%B3%95%E6%94%B6%E6%95%9B%E8%BF%87%E7%A8%8B%E7%A4%BA%E6%84%8F%E5%9B%BE-1680230431771-16.png)

最速下降法在收敛中其**早期的迭代步骤**往往在沿着同一个方向（如上图，第一步和第三步方向相同）。

如果我们每次迭代前，都能选择正确的方向迈出去，效果不是会更好吗？

方法：选择一系列相互正交的搜索方向，$\vec{d_{(0)}}, \vec{d_{(1)}}, \cdots, \vec{d_{(n-1)}}$，每个搜索方向只移动一步，而该步的长度刚好对齐到最小值点$\vec{x}$其实就是刚好能够**消除误差向量在这个方向上的分量**），$n$步之后，迭代结束，到达最小值点。

![img](./imags/%E6%9C%80%E9%80%9F%E4%B8%8B%E9%99%8D%E6%B3%95%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E7%A4%BA%E6%84%8F%E5%9B%BE.png)

注意：第一轮迭代后的误差向量$\vec{e_1}$与第一步搜索方向$\vec{d_0}$正交（如上图）。
$$
\vec{x_{(i+1)}} = \vec{x_{(i)}} + \alpha_{(i)} \cdot \vec{d_{(i)}} 
\tag{7 - 1}
$$
为了确定$\alpha$的值，需要用误差向量$\vec{e_{(i+1)}}$与搜索方向$\vec{d_{(i)}}$正交这一关系，同时避免后面继续搜索方向$\vec{d_{(i)}}$。这样得到
$$
\begin{split} \vec{d_{(i)}}^T \vec{e_{(i+1)}} &= 0 \\ \vec{d_{(i)}}^T ( \vec{e_{(i)}} + \alpha_{(i)} \vec{d_{(i)}}) &= 0 \qquad (据式7-1)\\ \alpha_{(i)} &= -\frac{\vec{d_{(i)}}^T \vec{e_{(i)}}}  {\vec{d_{(i)}}^T \vec{d_{(i)}}} \end{split} \tag{7 - 2}
$$
但是由于误差向量$\vec{e_{(i)}}$未知，还无法求出$\alpha$。

解决方法是用**矩阵·向量正交(A-orthogonal)**替代**向量正交(orthogonal)**。所谓的矩阵·向量正交（或共轭），是指存在两个向量$\vec{d_{(i)}}、\vec{d_{(j)}}$，满足：
$$
\vec{d_{(i)}}^T \textbf{A} \vec{d_{(j)}} = 0 , \quad i \ne j \tag{7 - 3}
$$
**图形解释**：

变换前不正交：

![img](./imags/%E4%B8%8E%E7%9F%A9%E9%98%B5%E6%AD%A3%E4%BA%A4%E7%9A%84%E5%90%91%E9%87%8F%E7%A4%BA%E6%84%8F%E5%9B%BE.png)

经过（A）变换后正交了：

![img](./imags/%E5%8F%98%E6%8D%A2%E8%BF%87%E5%90%8E%E7%9A%84%E4%B8%8E%E7%9F%A9%E9%98%B5%E6%AD%A3%E4%BA%A4%E7%9A%84%E5%90%91%E9%87%8F%E5%AF%B9%E7%A4%BA%E6%84%8F%E5%9B%BE.png)

由此我们的前提条件就不再是要求**搜索方向两两正交**，而是变成了本轮迭代后的误差向量和上轮的搜索方向向量两两矩阵正交即$\vec{d_{(i)}} \textbf{A} \vec{e_{(i+1)}} = 0$。

**这种矩阵正交性条件刚好与最速下降法中沿着搜索方向找寻最小值点等价**。

证明如下：
$$
\begin{split} \frac{d}{d_\alpha} f(\vec{x_{(i+1)}}) &= f’(\vec{x_{(i+1)}})^T \frac{d}{d_\alpha} \vec{x_{(i+1)}} &\qquad (据式4-3)\\ &= -\vec{r_{(i+1)}}^T \vec{d_{(i)}} \\ &= -(\vec{d_{(i)}}^T \vec{r_{(i+1)}})^T \\ &= （\vec{d_{(i)}}^T \textbf{A} \vec{e_{(i+1)}})^T = 0 &\qquad (据式6-4) \\ \therefore \quad \vec{d_{(i)}}^T \textbf{A} \vec{e_{(i+1)}} &= 0 \end{split}  \tag{7 - 4}
$$
由此得到：
$$
\begin{split} \alpha_{(i)} &= -\frac{\vec{d_{(i)}}^T \textbf{A} \vec{e_{(i)}}} {\vec{d_{(i)}}^T \textbf{A} \vec{d_{(i)}}} \\ &= \frac{\vec{d_{(i)}}^T \vec{r_{(i)}}} {\vec{d_{(i)}}^T \textbf{A} \vec{d_{(i)}}} \end{split} \tag{7 - 5}
$$

#### 格莱姆-施密特共轭

由上节可知，要求的目标值，我们现在唯一需要的就是一组**矩阵·向量**正交的搜索方向向量${ \vec{d_{(i)}} }$的集合。幸运的是，存在一种非常简单的方法计算这些向量，这种方法我们称之为**[共轭格莱姆-施密特过程](https://en.wikipedia.org/wiki/Gram–Schmidt_process)**。

### 共轭梯度法

**CG**算法就是搜索方向由残差的共轭（即令$\vec{u_i} = \vec{r_{(i)}}$）组成的共轭方向法。

我们首先来考虑选择残差的共轭构建搜索方向的具体实现。由于搜索向量由残差向量构建，因此张成子空间$\mathcal{D}_i$即为$\lbrace \vec{r_{(0)}}, \vec{r_{(1)}}, \cdots, \vec{r_{(i-1)}} \rbrace$。

这个优点也让**CG**算法在每轮迭代中的空间和时间复杂度均从$\mathcal{O}(n^2)$变到$\mathcal{O}(m)$，其中$m$是矩阵$A$中的非零项数量。由于每一个残差向量都与它之前迭代过程中的搜索方向向量正交，因此它也必然与之前的残差向量正交，如下图所示

![img](./imags/Figure29.png)

![image-20221028151016330](./imags/image-20221028151016330.png)

此时：
$$
\vec{r_{(i)}}^T \vec{r_{(j)}} = 0, \qquad i \neq j  \tag{8 - 1}
$$
中间过程不太懂，略过……

#### 算法公式

$$
\begin{split} \vec{d_{(0)}} &= \vec{r_{(0)}} = \vec{b_{(0)}} - \textbf{A} \vec{x_{(0)}} \\ \alpha_{(i)} &= \frac{\vec{r_{(i)}}^T \vec{r_{(i)}}} {\vec{d_{(i)}}^T \textbf{A} \vec{d_{(i)}}} \qquad (据式) \\ \vec{x_{(i+1)}} &= \vec{x_{(i)}} + \alpha_{(i)} \vec{d_{(i)}} \\ \vec{r_{(i+1)}} &= \vec{r_{(i)}} - \alpha_{(i)} \textbf{A} \vec{d_{(i)}} \\ \beta_{(i+1)} &= \frac{\vec{r_{(i+1)}}^T \vec{r_{(i+1)}}} {\vec{r_{(i)}}^T \vec{r_{(i)}}} \\ \vec{d_{(i+1)}} &= \vec{r_{(i+1)}} + \beta_{(i+1)} \vec{d_{(i)}} \end{split} \tag{8 - 6}
$$



![img](./imags/Figure30.png)

#### 对比

- 共轭梯度法比最速下降法更省内存（不需要存储前一次迭代的信息）

- 一般来说会在 n 步内终止，其中 n 是矩阵 A 的维数，但由于舍入误差的影响，迭代次数会比 n 多，因此常作为迭代法使用，但是也会在较快的有限步内终止！





[共轭梯度法通俗讲义 | 断鸿声里，立尽斜阳 (flat2010.github.io)](https://flat2010.github.io/2018/10/26/共轭梯度法通俗讲义/#译者后记)



参考：[共轭梯度法通俗讲义 | 断鸿声里，立尽斜阳 (flat2010.github.io)](https://flat2010.github.io/2018/10/26/共轭梯度法通俗讲义/#译者后记)
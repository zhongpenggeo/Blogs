```python
def gra_fun(X,w):
    #w=tf.constant([[2.0],[1.0]])
    y=tf.matmul(X,w)
    #y=tf.sin(tf.matmul(X,w))
    return tf.reduce_sum(y)
    
x=tf.constant([[4.0],[3.0],[2.0],[1.0]])
t=tf.constant([[4.0],[3.0],[2.0],[1.0]])*2.0
X= tf.concat([x,t],1)

w=tf.constant([[2.0],[1.0]])
#X=X**4
#y=tf.sin(tf.matmul(X,w))

with tf.GradientTape() as tape:
    tape.watch(w)
    # Packing together the inputs
    X_f = tf.stack([x[:, 0], t[:, 0]], axis=1)


    u = gra_fun(X_f,w)

    u_w = tape.gradient(u, w)
```
> tf.Tensor(
[[10.]
 [20.]], shape=(2, 1), dtype=float32)

结果：y对w的导数是[4个x的和，4个t的和]

也就是说：<u>导数是累加的，和输入的sample数量相关</u>



更新，如果y是求得平均值（上面示例用的求和）

那么结果是：

> ```
> tf.Tensor(
> [[2.5]
>  [5. ]], shape=(2, 1), dtype=float32)
> ```

也就是输入样本的和



so
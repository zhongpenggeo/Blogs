## Variable

TensorFlow **变量**是表示程序操作的**共享持久状态**的最佳方式。

变量是通过`tf.Variable`类操纵的。 `tf.Variable`代表张量，通过运行op可以改变它的值。与`tf.Tensor`对象不同， `tf.Variable`存在于单个`session.run`调用的上下文之外。

在内部，`tf.Variable`存储持久张量。具体操作允许您读取和修改张量的值。这些修改在多个`tf.Session`中可见，因此多个工作人员可以看到`tf.Variable`的值相同。

### 1. 创建一个变量

`tf.get_variable`



**变量集合**

您也可以使用自己的收藏。任何字符串都是有效的集合名称，并且不需要显式创建集合。要在创建变量后向变量添加变量（或任何其他对象），请调用`tf.add_to_collection`。例如，下面的代码将一个已命名的变量`my_local`添加到名为的集合中`my_collection_name`：

```python
tf.add_to_collection("my_collection_name", my_local)
```

并且可以检索已放置在集合中的所有变量（或其他对象）的列表，您可以使用：

### 2. 初始化


#### 一次性初始化
要在训练开始前一次初始化所有可训练变量，请调用`tf.global_variables_initializer()`。这个函数返回一个负责初始化`tf.GraphKeys.GLOBAL_VARIABLES`集合中所有变量的操作。运行此操作会初始化所有变量。例如：

```python
session.run(tf.global_variables_initializer())
# Now all variables are initialized.
```
请注意，默认情况下tf.global_variables_initializer，不会指定变量初始化的顺序。因此，如果变量的初始值取决于另一个变量的值，那么很可能会出现错误。
任何时候，如果在并非所有变量都被初始化的上下文中使用变量的值（例如，如果在初始化另一个变量时使用变量的值），则最好使用`variable.initialized_value()`而不是`variable`：

```python
v = tf.get_variable("v", shape=(), initializer=tf.zeros_initializer())
w = tf.get_variable("w", initializer=v.initialized_value() + 1)
```
#### 自定义初始化单个变量
你可以运行变量的初始化操作。例如：

```python
session.run(my_variable.initializer)
```
### 3. 使用变量

要使用`tf.Variable`TensorFlow图中的值，只需将其视为普通`tf.Tensor`：

```python
v = tf.get_variable("v", shape=(), initializer=tf.zeros_initializer())
w = v + 1  # w is a tf.Tensor which is computed based on the value of v.
           # Any time a variable is used in an expression it gets automatically
           # converted to a tf.Tensor representing its value.
```

#### 4. 共享变量

TensorFlow支持两种共享变量的方式：

- 显式传递`tf.Variable`对象。

- `tf.Variable`在`tf.variable_scope`对象中隐式地包装对象。

虽然显式传递变量的代码非常清晰，但编写TensorFlow函数实现中隐式使用变量有时很方便。大多数功能层都`tf.layer`使用这种方法，以及所有`tf.metrics`和其他一些库工具。

变量作用域允许您在调用隐式创建和使用变量的函数时控制变量重用。它们还允许您以分层和可理解的方式命名变量。

例如，假设我们编写一个函数来创建一个卷积/ relu层：

```python
def conv_relu(input, kernel_shape, bias_shape):
    # Create variable named "weights".
    weights = tf.get_variable("weights", kernel_shape,
        initializer=tf.random_normal_initializer())
    # Create variable named "biases".
    biases = tf.get_variable("biases", bias_shape,
        initializer=tf.constant_initializer(0.0))
    conv = tf.nn.conv2d(input, weights,
        strides=[1, 1, 1, 1], padding='SAME')
    return tf.nn.relu(conv + biases)
```

该功能使用短名称`weights`和`biases`，这有利于清晰区分。然而，在一个真实的模型中，我们需要许多这样的卷积图层，并且重复调用这个函数将不起作用：

```python
input1 = tf.random_normal([1,10,10,32])
input2 = tf.random_normal([1,20,20,32])
x = conv_relu(input1, kernel_shape=[5, 5, 32, 32], bias_shape=[32])
x = conv_relu(x, kernel_shape=[5, 5, 32, 32], bias_shape = [32])  # This fails.
```

由于期望的行为不清楚（创建新的变量或重新使用现有的变量？）TensorFlow将失败。`conv_relu`然而，调用不同的范围会说明我们想要创建新的变量：

```python
def my_image_filter(input_images):
    with tf.variable_scope("conv1"):
        # Variables created here will be named "conv1/weights", "conv1/biases".
        relu1 = conv_relu(input_images, [5, 5, 32, 32], [32])
    with tf.variable_scope("conv2"):
        # Variables created here will be named "conv2/weights", "conv2/biases".
        return conv_relu(relu1, [5, 5, 32, 32], [32])
```

如果你想要共享变量，你有两个选择。首先，您可以使用以下命令创建具有相同名称的作用域`reuse=True`：

```python
with tf.variable_scope("model"):
  output1 = my_image_filter(input1)
with tf.variable_scope("model", reuse=True):
  output2 = my_image_filter(input2)
```

您也可以调用`scope.reuse_variables()`以触发重复使用：

```python
with tf.variable_scope("model") as scope:
  output1 = my_image_filter(input1)
  scope.reuse_variables()
  output2 = my_image_filter(input2)
```

由于取决于范围的确切字符串名称可能会很危险，因此也可以基于另一个范围初始化变量作用域：

```python
with tf.variable_scope("model") as scope:
  output1 = my_image_filter(input1)
with tf.variable_scope(scope, reuse=True):
  output2 = my_image_filter(input2)
```